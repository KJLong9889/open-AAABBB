import pandas as pd
import numpy as np
import time
import os
import shutil
from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor
from concurrent.futures import ProcessPoolExecutor, as_completed
import logging
logging.getLogger('autogluon').setLevel(logging.ERROR)   # åªæ‰“ ERROR
import os



os.environ["TABPFN_MODEL_CACHE_DIR"] = "checkpoints"
# ====================== è·¯å¾„é…ç½® ======================
INPUT_HISTORY_DIR = "data2"
OUTPUT_DIR = "results_v2/"
MODEL_PATH_TEMPLATE = "AutogluonModels/AutogluonModels_{filename}"
METRICS_FILE = "evaluation_metrics1.csv"  # æ–°å¢ï¼šè¯„ä¼°æŒ‡æ ‡æ–‡ä»¶

FORCE_DELETE_MODEL = True
TEST_DAYS = 7  # æµ‹è¯•é›†é•¿åº¦ = é¢„æµ‹é•¿åº¦
PREDICTION_LENGTH = TEST_DAYS

# ====================== æ ¸å¿ƒå­—æ®µ ======================
ID_COLUMN = "item_id"
TIMESTAMP_COLUMN = "date"
TARGET_COLUMN = "value"

# ====================== å·²çŸ¥æœªæ¥åå˜é‡ ======================
KNOWN_COVARIATES = [
    'precip', 'windmax', 'windmaxdir', 'rhhi',
    'temphi', 'templo', 'avgtemp',
    'day_of_week', 'is_weekend',
    'week_sin', 'week_cos',
    'month_sin', 'month_cos',
    'days_since_start','month'
]


# ====================== æ—¶é—´ç‰¹å¾å‡½æ•° ======================
def add_time_features(df, timestamp_col='date'):
    df = df.copy()
    df[timestamp_col] = pd.to_datetime(df[timestamp_col])
    df = df.sort_values(timestamp_col)

    df['month'] = df[timestamp_col].dt.month
    df['day_of_week'] = df[timestamp_col].dt.weekday
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

    base_date = df[timestamp_col].min()
    df['days_since_start'] = (df[timestamp_col] - base_date).dt.days

    df['week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)


    return df


# ====================== å•æ–‡ä»¶å¤„ç† ======================
def process_file(history_file):
    base_name = os.path.splitext(history_file)[0]

    try:
        # ---------- è¯»å–å†å²æ•°æ® ----------
        df = pd.read_csv(os.path.join(INPUT_HISTORY_DIR, history_file))
        df[ID_COLUMN] = base_name
        df = add_time_features(df, TIMESTAMP_COLUMN)

        # ---------- å­—æ®µæ£€æŸ¥ ----------
        required_cols = [ID_COLUMN, TIMESTAMP_COLUMN, TARGET_COLUMN] + KNOWN_COVARIATES
        missing = [c for c in required_cols if c not in df.columns]
        if missing:
            return f"âŒ {history_file} ç¼ºå°‘åˆ—: {missing}", None

        df = df.sort_values(TIMESTAMP_COLUMN)

        if len(df) <= TEST_DAYS:
            return f"âš ï¸ {history_file} æ•°æ®é‡ä¸è¶³", None

        # ======================================================
        # âœ… æ ¸å¿ƒæ”¹åŠ¨ï¼šæ—¶é—´åˆ‡åˆ†
        # è®­ç»ƒé›†ï¼šè¿‡å»
        # æµ‹è¯•é›†ï¼šçœŸå®æœªæ¥
        # ======================================================
        df_train = df.iloc[:-TEST_DAYS].copy()
        df_test = df.iloc[-TEST_DAYS:].copy()

        print(f"ğŸ“Š {history_file} | Train: {len(df_train)} | Test(Future): {len(df_test)}")

        # ---------- æ¸…ç†æ¨¡å‹è·¯å¾„ ----------
        model_path = MODEL_PATH_TEMPLATE.format(filename=base_name)
        if FORCE_DELETE_MODEL and os.path.exists(model_path):
            shutil.rmtree(model_path)

        # ---------- æ„é€ è®­ç»ƒ TSDF ----------
        train_tsdf = TimeSeriesDataFrame.from_data_frame(
            df_train,
            id_column=ID_COLUMN,
            timestamp_column=TIMESTAMP_COLUMN
        )

        # ---------- æ„é€ é¢„æµ‹å™¨ ----------
        predictor = TimeSeriesPredictor(
            prediction_length=PREDICTION_LENGTH,
            freq='D',
            target=TARGET_COLUMN,
            known_covariates_names=KNOWN_COVARIATES,
            eval_metric="WQL",
            eval_metric_seasonal_period=1,
            quantile_levels=[0.3, 0.5, 0.7],
            path=model_path
        )

        # ---------- æ¨¡å‹è®­ç»ƒ ----------
        predictor.fit(
            train_data=train_tsdf,
            time_limit=1200,
            enable_ensemble=True,
            hyperparameters={
                # 1. æ‚¨åŸæœ‰çš„åŸºäºè¡¨æ ¼çš„æ¨¡å‹ (å°†æ—¶åºè½¬åŒ–ä¸ºå›å½’é—®é¢˜)
                "DirectTabular": {
                    "tabular_hyperparameters": {
                        "TABPFNV2": {}  # ä½¿ç”¨ TabPFN ä½œä¸ºåº•å±‚å›å½’å™¨
                    }
                },

                # 2. æ·±åº¦å­¦ä¹ æ¨¡å‹ (é€šå¸¸æ•ˆæœæœ€å¥½ï¼Œé€‚åˆæ•æ‰å¤æ‚æ¨¡å¼)
                "DeepAR": {},  # ç»å…¸çš„æ¦‚ç‡é¢„æµ‹æ¨¡å‹ï¼Œç¨³å¥æ€§å¥½
                "PatchTST": {},  # åŸºäº Transformer çš„æœ€æ–° SOTA æ¨¡å‹ï¼Œæ“…é•¿é•¿åºåˆ—
                "TiDE": {},  # è°·æ­Œæ¨å‡ºçš„åŸºäº MLP çš„æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ä¸”æ•ˆæœå¥½
            }
        )

        # ======================================================
        # âœ… æ ¸å¿ƒæ”¹åŠ¨ï¼šæœªæ¥åå˜é‡ = df_test
        # ======================================================
        future_cov = df_test[
            [ID_COLUMN, TIMESTAMP_COLUMN] + KNOWN_COVARIATES
            ].copy()

        future_tsdf = TimeSeriesDataFrame.from_data_frame(
            future_cov,
            id_column=ID_COLUMN,
            timestamp_column=TIMESTAMP_COLUMN
        )

        # ---------- é¢„æµ‹ ----------
        predictions = predictor.predict(
            data=train_tsdf,
            known_covariates=future_tsdf
        )

        # ---------- ç»“æœå¤„ç† ----------
        pred_df = predictions.reset_index()

        if "timestamp" in pred_df.columns and TIMESTAMP_COLUMN != "timestamp":
            pred_df.rename(columns={"timestamp": TIMESTAMP_COLUMN}, inplace=True)

            # åŒæ ·ï¼Œå¦‚æœ ID åˆ—åä¸ä¸€è‡´ï¼Œä¹Ÿå¯ä»¥å®‰å…¨åœ°æ”¹å›æ¥ï¼ˆè™½ç„¶ item_id ä¸å½±å“ mergeï¼‰
        if "item_id" in pred_df.columns and ID_COLUMN != "item_id":
            pred_df.rename(columns={"item_id": ID_COLUMN}, inplace=True)

        pred_df[TIMESTAMP_COLUMN] = pd.to_datetime(pred_df[TIMESTAMP_COLUMN])
        df_test[TIMESTAMP_COLUMN] = pd.to_datetime(df_test[TIMESTAMP_COLUMN])
        # åˆå¹¶çœŸå®å€¼
        pred_df = pred_df.merge(
            df_test[[TIMESTAMP_COLUMN, TARGET_COLUMN]],
            on=TIMESTAMP_COLUMN,
            how='left'
        )

        pred_df.rename(columns={TARGET_COLUMN: "actual"}, inplace=True)
        pred_df['error'] = pred_df['0.5'] - pred_df['actual']
        pred_df['abs_error'] = pred_df['error'].abs()

        # ---------- è¯„ä¼° ----------
        mae = pred_df['abs_error'].mean()
        rmse = np.sqrt((pred_df['error'] ** 2).mean())
        mape = (pred_df['abs_error'] / pred_df['actual']).mean() * 100

        # ---------- ä¿å­˜ ----------
        output_file = os.path.join(OUTPUT_DIR, f"{base_name}_forecast.csv")
        pred_df.to_csv(output_file, index=False, encoding='utf-8-sig')

        # è¿”å›è¯„ä¼°æŒ‡æ ‡
        metrics = {
            'filename': history_file,
            'mae': mae,
            'rmse': rmse,
            'mape': mape
        }

        return (
            f"âœ… {history_file} å®Œæˆ | "
            f"MAE={mae:.2f} RMSE={rmse:.2f} MAPE={mape:.2f}%"
        ), metrics

    except Exception as e:
        import traceback
        error_msg = f"âŒ {history_file} å¤±è´¥\n{traceback.format_exc()[:300]}"
        return error_msg, None


# ====================== å¹¶è¡Œå¤„ç† ======================
def batch_process_parallel(max_workers=4):
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    files = [f for f in os.listdir(INPUT_HISTORY_DIR) if f.endswith(".csv")]

    all_metrics = []  # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶çš„è¯„ä¼°æŒ‡æ ‡

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_file, f): f for f in files}
        for future in as_completed(futures):
            result, metrics = future.result()
            print(result)

            if metrics is not None:
                all_metrics.append(metrics)

    # ä¿å­˜æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡åˆ°CSVæ–‡ä»¶
    if all_metrics:
        metrics_df = pd.DataFrame(all_metrics)
        metrics_df = metrics_df[['filename', 'mae', 'rmse', 'mape']]  # ç¡®ä¿åˆ—é¡ºåº
        metrics_path = os.path.join(OUTPUT_DIR, METRICS_FILE)
        metrics_df.to_csv(metrics_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“Š è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")
        print(f"å…±å¤„ç† {len(metrics_df)} ä¸ªæ–‡ä»¶")
        print("\nè¯„ä¼°æŒ‡æ ‡æ‘˜è¦:")
        print(metrics_df)
    else:
        print("âš ï¸ æœªç”Ÿæˆä»»ä½•è¯„ä¼°æŒ‡æ ‡")


if __name__ == "__main__":
    batch_process_parallel(max_workers=4)
    print("ğŸ“¢ å…¨éƒ¨å¤„ç†å®Œæˆ")