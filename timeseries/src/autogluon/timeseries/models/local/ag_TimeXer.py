import pandas as pd
import numpy as np
import time
import os
import sys
import shutil
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç»å¯¹è·¯å¾„ (å³é¡¹ç›®çš„æ ¹ç›®å½•)
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# æ‰‹åŠ¨æ·»åŠ å„å­æ¨¡å—çš„ src ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'common', 'src'))
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'core', 'src'))
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'features', 'src'))
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'timeseries', 'src'))

from autogluon.timeseries import TimeSeriesDataFrame
from autogluon.timeseries import TimeSeriesPredictor
from autogluon.timeseries.utils.features import TimeSeriesFeatureGenerator

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.getLogger('autogluon').setLevel(logging.WARNING)

os.environ["TABPFN_MODEL_CACHE_DIR"] = "checkpoints"

# ====================== è·¯å¾„é…ç½® ======================
INPUT_HISTORY_DIR = "data1"
OUTPUT_DIR = "results/forecast_future"  # ä¿®æ”¹è¾“å‡ºç›®å½•ä»¥ç¤ºåŒºåˆ«
MODEL_PATH_TEMPLATE = "AutogluonModels/AutogluonModels_{filename}"
STATUS_FILE = "prediction_status.csv"   # çŠ¶æ€è®°å½•

FORCE_DELETE_MODEL = True
PREDICTION_LENGTH = 7  # é¢„æµ‹æœªæ¥å¤šå°‘å¤©

# ====================== æ ¸å¿ƒå­—æ®µ ======================
ID_COLUMN = "item_id"
TIMESTAMP_COLUMN = "date"
TARGET_COLUMN = "value"

# ====================== å·²çŸ¥æœªæ¥åå˜é‡ ======================
# è®¾ä¸ºç©ºï¼Œä¸å†ä½¿ç”¨æœªæ¥å¤©æ°”ç­‰æ•°æ®
KNOWN_COVARIATES = [] 

# ====================== é›†æˆç­–ç•¥é…ç½®å‡½æ•° (å·²ä¿®å¤) ======================
def get_ensemble_configs(ensemble_types):
    """
    æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„ç±»å‹åˆ—è¡¨ï¼Œç”Ÿæˆ AutoGluon çš„ ensemble_hyperparameters
    
    æ”¯æŒç±»å‹: 
      - 'weighted': å…¨å±€è´ªå¿ƒåŠ æƒ (æœ€å¿«ï¼Œæœ€ç¨³)
      - 'per_item': åˆ†ç‰©å“è´ªå¿ƒåŠ æƒ (é€‚åˆä¸åŒå•†å“è§„å¾‹å·®å¼‚å¤§çš„æƒ…å†µ)
      - 'stacking': å…¨å±€å †å  (Tabular Stacking)
      - 'simple':   ç®€å•å¹³å‡
      - 'median':   ä¸­ä½æ•°é›†æˆ
      - 'quantile': åˆ†ä½æ•°å †å  (ææ…¢ï¼Œæ…ç”¨)
    """
    ensemble_hps = {}
    
    # 1. å…¨å±€åŠ æƒé›†æˆ (WeightedEnsemble / Greedy) - åŸºç¡€å¿…å¤‡
    if 'weighted' in ensemble_types:
        ensemble_hps["WeightedEnsemble"] = {"max_models": 25} 
        
    # 2. åˆ†ç‰©å“åŠ æƒé›†æˆ (PerItemGreedyEnsemble) - è¿›é˜¶æ¨è
    # å¦‚æœä½ çš„å•†å“æœ‰äº›æ˜¯ DeepAR å‡†ï¼Œæœ‰äº›æ˜¯ AutoETS å‡†ï¼Œè¿™ä¸ªæ¨¡å‹ä¼šè‡ªåŠ¨åˆ‡æ¢
    if 'per_item' in ensemble_types:
        ensemble_hps["PerItemGreedyEnsemble"] = {"max_models": 25}

    # 3. å †å é›†æˆ (TabularEnsemble / Stacking)
    if 'stacking' in ensemble_types:
        # æ³¨æ„ï¼šå¦‚æœç‰¹å¾è¿‡å¤š(å¦‚å¼€äº†TSfresh)ï¼Œè¿™é‡Œå®¹æ˜“æŠ¥ç»´åº¦é”™è¯¯
        ensemble_hps["TabularEnsemble"] = {
            "model_name": "CAT",  # ä½¿ç”¨ CatBoost
            "max_num_samples": 100000 
        }
        
    # 4. ç®€å•å¹³å‡ (SimpleAverage)
    if 'simple' in ensemble_types:
        ensemble_hps["SimpleAverage"] = {}

    # 5. ä¸­ä½æ•°é›†æˆ (MedianEnsemble)
    if 'median' in ensemble_types:
        ensemble_hps["MedianEnsemble"] = {}
    
    # 6. åˆ†ä½æ•°å †å  (PerQuantileTabularEnsemble) - è®¡ç®—é‡å·¨å¤§
    if 'quantile' in ensemble_types:
        ensemble_hps["PerQuantileTabularEnsemble"] = {
            "model_name": "CAT",
            "max_num_samples": 50000
        }
        
    return ensemble_hps

# ====================== æ—¶é—´ç‰¹å¾å‡½æ•° ======================
def add_time_features(df, timestamp_col='date'):
    df = df.copy()
    df[timestamp_col] = pd.to_datetime(df[timestamp_col])
    df = df.sort_values(timestamp_col)
    return df

# ====================== å•æ–‡ä»¶å¤„ç† ======================
def process_file(history_file, ensemble_types=["weighted"]):
    base_name = os.path.splitext(history_file)[0]
    item_output_dir = os.path.join(OUTPUT_DIR, base_name)
    os.makedirs(item_output_dir, exist_ok=True)

    try:
        print(f"[Start] æ­£åœ¨å¤„ç†: {base_name} | é›†æˆç­–ç•¥: {ensemble_types}")

        # ---------- è¯»å–å…¨é‡å†å²æ•°æ® ----------
        df = pd.read_csv(os.path.join(INPUT_HISTORY_DIR, history_file))
        df[ID_COLUMN] = base_name
        df = add_time_features(df, TIMESTAMP_COLUMN)

        # ---------- å­—æ®µæ£€æŸ¥ ----------
        required_cols = [ID_COLUMN, TIMESTAMP_COLUMN, TARGET_COLUMN] + KNOWN_COVARIATES
        missing = [c for c in required_cols if c not in df.columns]
        if missing:
            return f"âŒ {history_file} ç¼ºå°‘åˆ—: {missing}", None

        df = df.sort_values(TIMESTAMP_COLUMN)

        # ======================================================
        # å…¨é‡æ•°æ®æ„å»º TimeSeriesDataFrame
        # ======================================================
        full_tsdf = TimeSeriesDataFrame.from_data_frame(
            df,
            id_column=ID_COLUMN,
            timestamp_column=TIMESTAMP_COLUMN
        )


        # ======================================================
        # ğŸ—ï¸ Block 1 & 2: ç»Ÿè®¡ä¸å¼‚è´¨æ€§åˆ†æ (åŸºäºå…¨é‡æ•°æ®)
        # ======================================================
        print(f"[Analysis] æ•°æ®åˆ†æ (Full Data)...")
        full_tsdf._compute_missing_rate_KANG(save_dir=item_output_dir, save=True)
        full_tsdf._plot_distribution_KANG(save_dir=item_output_dir, save=True)
        full_tsdf._spatiotemporal_heterogeneity_analysis_KANG(
            target=TARGET_COLUMN,
            save_dir=item_output_dir,
            save=True
        )

        # ======================================================
        # Block 3: ç‰¹å¾å·¥ç¨‹åˆ†æ
        # ======================================================
        print(f" [Feature] ç‰¹å¾å·¥ç¨‹åˆ†æ...")
        analysis_feature_generator = TimeSeriesFeatureGenerator(
            target=TARGET_COLUMN,
            known_covariates_names=KNOWN_COVARIATES,
            use_tsfresh=False,            
            correlation_analysis=True,    
            correlation_method="pearson",
            correlation_output_dir=os.path.join(item_output_dir, "correlation_analysis"),
            max_correlation_features=50
        )
        full_tsdf = analysis_feature_generator.fit_transform(full_tsdf)

        # ======================================================
        # Block 4: æ¨¡å‹è®­ç»ƒæœåŠ¡
        # ======================================================
        print(f"[Train] æ¨¡å‹å…¨é‡è®­ç»ƒ...")

        model_path = MODEL_PATH_TEMPLATE.format(filename=base_name)
        if FORCE_DELETE_MODEL and os.path.exists(model_path):
            shutil.rmtree(model_path)

        predictor = TimeSeriesPredictor(
            prediction_length=PREDICTION_LENGTH,
            freq='D',
            target=TARGET_COLUMN,
            known_covariates_names=KNOWN_COVARIATES,
            eval_metric="MSE", 
            path=model_path,
            use_tsfresh=False, 
            correlation_analysis=False,
            verbosity=3
        )

        # --- è·å–ä¿®æ­£åçš„é›†æˆé…ç½® ---
        ensemble_hps = get_ensemble_configs(ensemble_types)


        predictor.fit(
            train_data=full_tsdf,
            time_limit=1200, 
            enable_ensemble=True,
            ensemble_hyperparameters=ensemble_hps, 
            num_val_windows=1,
            presets="fast_training",           
            hyperparameters={
                "TimeXer": {}
            }
        )
        
        # ======================================================
        # New Block: å¯¼å‡ºè®­ç»ƒè¯¦æƒ… (Leaderboard & Val Predictions)
        # ======================================================
        print(f"ğŸ“ [Export] å¯¼å‡ºæ¨¡å‹æŒ‡æ ‡ä¸éªŒè¯é›†é¢„æµ‹...")
        
        # 1. å¯¼å‡º Leaderboard
        leaderboard_df = predictor.leaderboard(data=full_tsdf, 
                                               silent=True, 
                                               extra_metrics=["MAE", "RMSE", "MAPE", "SMAPE", "RMSSE"]
                        )
        leaderboard_path = os.path.join(item_output_dir, f"{base_name}_model_leaderboard.csv")
        leaderboard_df.to_csv(leaderboard_path, index=False, encoding='utf-8-sig')

        # 2. å¯¼å‡ºæ‰€æœ‰æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„é¢„æµ‹å€¼
        val_input_data = full_tsdf.slice_by_timestep(None, -PREDICTION_LENGTH)
        
        model_names = predictor.model_names()
        
        all_val_preds = []
        for model_name in model_names:
            try:
                preds = predictor.predict(val_input_data, model=model_name)
                preds = preds.reset_index()
                preds['model'] = model_name 
                all_val_preds.append(preds)
            except Exception as e:
                logging.warning(f"æ¨¡å‹ {model_name} éªŒè¯é›†é¢„æµ‹å¤±è´¥: {e}")

        if all_val_preds:
            combined_val_preds = pd.concat(all_val_preds, ignore_index=True)
            
            if "timestamp" in combined_val_preds.columns and TIMESTAMP_COLUMN != "timestamp":
                combined_val_preds.rename(columns={"timestamp": TIMESTAMP_COLUMN}, inplace=True)
            if "item_id" in combined_val_preds.columns and ID_COLUMN != "item_id":
                combined_val_preds.rename(columns={"item_id": ID_COLUMN}, inplace=True)
                
            val_preds_path = os.path.join(item_output_dir, f"{base_name}_validation_predictions_all_models.csv")
            combined_val_preds.to_csv(val_preds_path, index=False, encoding='utf-8-sig')
        

        # ======================================================
        # Block 5: æ„é€ æœªæ¥æ•°æ®å¹¶æ¨ç† (Future Forecast)
        # ======================================================
        print(f"[Predict] æœªæ¥é¢„æµ‹...")

        last_date = df[TIMESTAMP_COLUMN].max()
        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=PREDICTION_LENGTH, freq='D')
        
        future_cov_df = pd.DataFrame({TIMESTAMP_COLUMN: future_dates})
        future_cov_df[ID_COLUMN] = base_name
        
        future_tsdf = TimeSeriesDataFrame.from_data_frame(
            future_cov_df,
            id_column=ID_COLUMN,
            timestamp_column=TIMESTAMP_COLUMN
        )

        predictions = predictor.predict(
            data=full_tsdf,
            known_covariates=future_tsdf
        )

        pred_df = predictions.reset_index()

        if "timestamp" in pred_df.columns and TIMESTAMP_COLUMN != "timestamp":
            pred_df.rename(columns={"timestamp": TIMESTAMP_COLUMN}, inplace=True)
        if "item_id" in pred_df.columns and ID_COLUMN != "item_id":
            pred_df.rename(columns={"item_id": ID_COLUMN}, inplace=True)

        output_file = os.path.join(item_output_dir, f"{base_name}_future_prediction.csv")
        pred_df.to_csv(output_file, index=False, encoding='utf-8-sig')

        status_info = {
            'filename': history_file,
            'status': 'Success',
            'start_date': future_dates[0].strftime('%Y-%m-%d'),
            'end_date': future_dates[-1].strftime('%Y-%m-%d'),
            'models_trained': len(model_names),
            'best_model': predictor.model_best
        }

        return (
            f"{history_file} å®Œæˆ | æœ€ä½³æ¨¡å‹: {predictor.model_best}"
        ), status_info

    except Exception as e:
        import traceback
        error_msg = f"{history_file} å¤±è´¥\n{traceback.format_exc()[:300]}"
        print(error_msg)
        return error_msg, None


# ====================== å¹¶è¡Œå¤„ç† ======================
def batch_process_parallel(max_workers=4):
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    if not os.path.exists(INPUT_HISTORY_DIR):
        print(f"æ‰¾ä¸åˆ°è¾“å…¥ç›®å½•: {INPUT_HISTORY_DIR}")
        return

    files = [f for f in os.listdir(INPUT_HISTORY_DIR) if f.endswith(".csv")]
    
    if not files:
        print(f"ç›®å½• {INPUT_HISTORY_DIR} ä¸­æ²¡æœ‰ CSV æ–‡ä»¶")
        return

    all_status = []
    
    # === åœ¨è¿™é‡Œå®šä¹‰ä½ æƒ³è¦è¿è¡Œçš„é›†æˆç±»å‹ ===
    USE_ENSEMBLES = ['weighted', 'stacking', 'simple', 'median', "per_item","quantile"] 
    
    print(f"å¼€å§‹å…¨é‡è®­ç»ƒä¸é¢„æµ‹ï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶...")
    print(f"å¯ç”¨çš„é›†æˆç­–ç•¥: {USE_ENSEMBLES}")

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_file, f, USE_ENSEMBLES): f for f in files}
        
        for future in as_completed(futures):
            result, status = future.result()
            print(result)

            if status is not None:
                all_status.append(status)

    if all_status:
        status_df = pd.DataFrame(all_status)
        status_path = os.path.join(OUTPUT_DIR, STATUS_FILE)
        status_df.to_csv(status_path, index=False, encoding='utf-8-sig')
        print(f"\né¢„æµ‹çŠ¶æ€è¡¨å·²ä¿å­˜åˆ°: {status_path}")
    else:
        print("æœªå®Œæˆä»»ä½•é¢„æµ‹")


if __name__ == "__main__":
    batch_process_parallel(max_workers=1)
    print("ä»»åŠ¡å…¨éƒ¨å®Œæˆ")
